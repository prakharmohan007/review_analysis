{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Ratings from Reviews Using Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file\n",
    "import pandas as pd\n",
    "df_orig = pd.read_csv(\"Reviews.csv\",engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) NB Classifier with Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each review is considered as a bag of words and a NB classifier is trained to predict the rating of the review. As the distribution of the ratings is not uniform, we use class weights to avoid the skew in the rating estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Text\n",
      "Score        \n",
      "1       52268\n",
      "2       29769\n",
      "3       42640\n",
      "4       80655\n",
      "5      363122\n"
     ]
    }
   ],
   "source": [
    "# Selct the Score and Text(review) columns from the whole data\n",
    "df = df_orig[['Score', 'Text']]\n",
    "# Remove the rows with no review\n",
    "df = df[pd.notnull(df['Text'])]\n",
    "# Find the number of samples for each score\n",
    "print(df.groupby('Score').count())\n",
    "df_final = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the sample count in each column \n",
    "weights = df.groupby('Score').count()\n",
    "# As the samples for score 5 are higher we use this sample count to normalize and determine sample weights\n",
    "class_weights =weights.loc[5,'Text']/ weights['Text']\n",
    "# Add a new column 'weight' for each sample/row\n",
    "def add_weight(row):\n",
    "    if row['Score'] == 1:\n",
    "        return class_weights.loc[1]\n",
    "    elif row['Score'] == 2:\n",
    "        return class_weights.loc[2]\n",
    "    elif row['Score'] == 3:\n",
    "        return class_weights.loc[3]\n",
    "    elif row['Score'] == 4:\n",
    "        return class_weights.loc[4]\n",
    "    else:\n",
    "        return class_weights.loc[5]\n",
    "df_final['weights'] = df_final.apply(lambda row: add_weight(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score\n",
       "1     6.947310\n",
       "2    12.197991\n",
       "3     8.515994\n",
       "4     4.502164\n",
       "5     1.000000\n",
       "Name: Text, dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Split the data into train and test\n",
    "X_train, X_test, y_train, y_test, W_train, W_test= train_test_split(df_final['Text'], df_final['Score'].astype(float), df_final['weights'], random_state = 0)\n",
    "# Find the tfidf vector representation for the train data\n",
    "count_vect = CountVectorizer(min_df=20,stop_words='english')\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "# Train a MultinomialNB classifier for the train data\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train, sample_weight= W_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for the predicted samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 8563,  2464,  1038,   406,   504],\n",
       "       [ 1711,  3361,  1493,   621,   435],\n",
       "       [ 1445,  1714,  4522,  1833,  1023],\n",
       "       [ 1424,  1328,  3212,  8980,  5400],\n",
       "       [ 4555,  3339,  5489, 16528, 60726]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the tfidf representation of the test data\n",
    "xtest = count_vect.transform(X_test)\n",
    "xidftext = tfidf_transformer.transform(xtest)\n",
    "# Predict the ratings for the test data\n",
    "y_pred = clf.predict(xidftext)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# generate a confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix for the predicted samples')\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Performance Metrics\n",
    "# Calculate accuracy\n",
    "corr_pred = 0\n",
    "for i in range(5):\n",
    "    corr_pred += conf_mat[i][i]\n",
    "accuracy = corr_pred/float(conf_mat.sum())\n",
    "# Calculate Mean Prediction Error \n",
    "y_true = y_test.to_numpy()\n",
    "d = {'y_true': y_true, 'y_pred': y_pred}\n",
    "results = pd.DataFrame(data=d)\n",
    "def find_diff(row):\n",
    "    return abs(row['y_pred']-row['y_true'])\n",
    "results['diff'] = results.apply(lambda row:find_diff(row), axis = 1)\n",
    "mean = results['diff'].mean()\n",
    "variance = results['diff'].var()\n",
    "# Find Precision & Recall\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "average_precision = precision_recall_fscore_support(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performace Metrics\n",
      "Accuracy =  0.6062175436621304\n",
      "Mean Prediction Error = 0.656451862589\n",
      "Variance of prediction Error =  1.0433243214\n",
      "Precision =  0.4509327320709585\n",
      "Recall = 0.5283066363397099\n"
     ]
    }
   ],
   "source": [
    "print 'Performace Metrics'\n",
    "print 'Accuracy = ', accuracy\n",
    "print 'Mean Prediction Error =', results['diff'].mean()\n",
    "print 'Variance of prediction Error = ', results['diff'].var()\n",
    "print 'Precision = ', average_precision[0]\n",
    "print 'Recall =', average_precision[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) NB Classifier with Unigrams & Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for the prediction samples\n",
      "[[ 9591  1672   922   378   412]\n",
      " [ 1492  3979  1250   524   376]\n",
      " [ 1134  1234  5660  1668   841]\n",
      " [ 1035   868  2312 11123  5006]\n",
      " [ 3150  1972  3807 13711 67997]]\n"
     ]
    }
   ],
   "source": [
    "# Find the vector representation by representing each review as bag of unigrams(single words) \\\n",
    "# and bigrams(pairs of adjacent word)\n",
    "count_vect = CountVectorizer(min_df=20,stop_words='english', ngram_range=(1, 2))\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "# Train a MultinomialNB classifier for the train data\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train, sample_weight= W_train)\n",
    "# Find the tfidf representation of the test data\n",
    "xtest = count_vect.transform(X_test)\n",
    "xidftext = tfidf_transformer.transform(xtest)\n",
    "# Predict the ratings for the test data\n",
    "y_pred = clf.predict(xidftext)\n",
    "# generate a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix for the prediction samples')\n",
    "print conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Performance Metrics\n",
    "# Calculate accuracy\n",
    "corr_pred = 0\n",
    "for i in range(5):\n",
    "    corr_pred += conf_mat[i][i]\n",
    "accuracy = corr_pred/float(conf_mat.sum())\n",
    "# Calculate Mean Prediction Error \n",
    "y_true = y_test.to_numpy()\n",
    "d = {'y_true': y_true, 'y_pred': y_pred}\n",
    "results = pd.DataFrame(data=d)\n",
    "def find_diff(row):\n",
    "    return abs(row['y_pred']-row['y_true'])\n",
    "results['diff'] = results.apply(lambda row:find_diff(row), axis = 1)\n",
    "mean = results['diff'].mean()\n",
    "variance = results['diff'].var()\n",
    "# Find Precision & Recall\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "average_precision = precision_recall_fscore_support(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performace Metrics\n",
      "Accuracy =  0.6920500443306078\n",
      "Mean Prediction Error = 0.493040798232\n",
      "Variance of prediction Error =  0.823454696964\n",
      "Precision =  0.5433179771015136\n",
      "Recall = 0.6190827707312356\n"
     ]
    }
   ],
   "source": [
    "print 'Performace Metrics'\n",
    "print 'Accuracy = ', accuracy\n",
    "print 'Mean Prediction Error =', results['diff'].mean()\n",
    "print 'Variance of prediction Error = ', results['diff'].var()\n",
    "print 'Precision = ', average_precision[0]\n",
    "print 'Recall =', average_precision[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Ratings from Review Summary Using Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for the predicted samples\n",
      "[[ 7415  1789  1249   401  2121]\n",
      " [ 2647  1388  1525   448  1613]\n",
      " [ 2389  1144  3062  1032  2910]\n",
      " [ 1717  1013  4146  2962 10506]\n",
      " [ 4422  2466  5496  7833 70420]]\n"
     ]
    }
   ],
   "source": [
    "# Read the csv file which has summary of the reviews\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"Reviews_summaries.csv\",engine='python')\n",
    "df.sample(frac=1)\n",
    "#df = df.sample(frac = 0.1, replace= False)\n",
    "df.head()\n",
    "df = df[['Score', 'predicted_summary']]\n",
    "df = df[pd.notnull(df['predicted_summary'])]\n",
    "df_final = df\n",
    "#Find the sample count in each column and find class weights w.r.t 5 class\n",
    "weights = df_final.groupby('Score').count()\n",
    "class_weights =weights.loc[5,'predicted_summary']/ weights['predicted_summary']\n",
    "# Add a new column 'weight' for each sample/row\n",
    "def add_weight(row):\n",
    "    if row['Score'] == 1:\n",
    "        return class_weights.loc[1]\n",
    "    elif row['Score'] == 2:\n",
    "        return class_weights.loc[2]\n",
    "    elif row['Score'] == 3:\n",
    "        return class_weights.loc[3]\n",
    "    elif row['Score'] == 4:\n",
    "        return class_weights.loc[4]\n",
    "    else:\n",
    "        return class_weights.loc[5]\n",
    "# Import the required libraries\n",
    "df_final['weights'] = df_final.apply(lambda row: add_weight(row), axis=1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Split the data into train and test\n",
    "X_train, X_test, y_train, y_test, W_train, W_test= train_test_split(df_final['predicted_summary'], \\\n",
    "                                                                    df_final['Score'].astype(float), \\\n",
    "                                                                    df_final['weights'], \\\n",
    "                                                                    random_state = 0)\n",
    "# Find the vector representation by representing each review summary as bag of unigrams(single words) \\\n",
    "# and bigrams(pairs of adjacent word)\n",
    "count_vect = CountVectorizer(min_df=20,stop_words='english', ngram_range=(1, 2))\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "# Train a MultinomialNB classifier for the train data\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train, sample_weight= W_train)\n",
    "# Find the tfidf representation of the test data\n",
    "xtest = count_vect.transform(X_test)\n",
    "xidftext = tfidf_transformer.transform(xtest)\n",
    "# Predict the ratings for the test data\n",
    "y_pred = clf.predict(xidftext)\n",
    "# generate a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix for the predicted samples')\n",
    "print conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Performance Metrics\n",
    "# Calculate accuracy\n",
    "corr_pred = 0\n",
    "for i in range(5):\n",
    "    corr_pred += conf_mat[i][i]\n",
    "accuracy = corr_pred/float(conf_mat.sum())\n",
    "# Calculate Mean Prediction Error \n",
    "y_true = y_test.to_numpy()\n",
    "d = {'y_true': y_true, 'y_pred': y_pred}\n",
    "results = pd.DataFrame(data=d)\n",
    "def find_diff(row):\n",
    "    return abs(row['y_pred']-row['y_true'])\n",
    "results['diff'] = results.apply(lambda row:find_diff(row), axis = 1)\n",
    "mean = results['diff'].mean()\n",
    "variance = results['diff'].var()\n",
    "# Find Precision & Recall\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "average_precision = precision_recall_fscore_support(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performace Metrics\n",
      "Accuracy =  0.5998494166654939\n",
      "Mean Prediction Error = 0.720513109194\n",
      "Variance of prediction Error =  1.20556219476\n",
      "Precision =  0.36249497146719023\n",
      "Recall = 0.39334962972340304\n"
     ]
    }
   ],
   "source": [
    "print 'Performace Metrics'\n",
    "print 'Accuracy = ', accuracy\n",
    "print 'Mean Prediction Error =', results['diff'].mean()\n",
    "print 'Variance of prediction Error = ', results['diff'].var()\n",
    "print 'Precision = ', average_precision[0]\n",
    "print 'Recall =', average_precision[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
