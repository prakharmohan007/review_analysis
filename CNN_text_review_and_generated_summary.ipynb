{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re, sys\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "import gensim\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Text reviews data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text_review = np.load('text_review.npy')\n",
    "ratings = np.load('ratings.npy')\n",
    "vocabulary_inv_text_review = np.load('text_review_vocabulary_inv.npy')\n",
    "with open('text_review_vocabulary.pkl', 'rb') as f:\n",
    "    vocabulary_text_review = pickle.load(f)\n",
    "vocabulary_inv_text_review = {rank: word for rank, word in enumerate(vocabulary_inv_text_review)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Generated summary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gen_summary = np.load('predicted_summary_x.npy')\n",
    "vocabulary_inv_gen_summary = np.load('predicted_summary_vocabulary_inv.npy')\n",
    "with open('predicted_summary_vocabulary.pkl', 'rb') as f:\n",
    "    vocabulary_gen_summary = pickle.load(f)\n",
    "vocabulary_inv_gen_summary = {rank: word for rank, word in enumerate(vocabulary_inv_gen_summary)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.load('train_indices.npy')\n",
    "test_indices = np.load('test_indices.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text_review = x_text_review[train_indices]\n",
    "x_test_text_review = x_text_review[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_gen_summary = x_gen_summary[train_indices]\n",
    "x_test_gen_summary = x_gen_summary[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings = ratings[train_indices]\n",
    "test_ratings = ratings[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length_text_review = x_test_text_review.shape[1]\n",
    "sequence_length_gen_summary = x_test_gen_summary.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot encoding on labels (ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_onehot(y):\n",
    "    res = [[0 for j in range(5)] for i in y]\n",
    "    \n",
    "    for index,i in enumerate(y):\n",
    "        res[index][i-1] = 1\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings = convert_to_onehot(train_ratings)\n",
    "test_ratings = convert_to_onehot(test_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_text_review shape: (454741, 3768)\n",
      "x_train_text_review shape: (454741, 7)\n",
      "train_ratings shape: (454741, 5)\n",
      "x_test_text_review shape: (113686, 3768)\n",
      "x_train_text_review shape: (113686, 7)\n",
      "test_ratings shape: (113686, 5)\n",
      "Vocabulary Size Text reviews: 127686\n",
      "Vocabulary Size Generated Summary: 537\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train_text_review shape:\", x_train_text_review.shape)\n",
    "print(\"x_train_gen_summary shape:\", x_train_gen_summary.shape)\n",
    "print(\"train_ratings shape:\", train_ratings.shape)\n",
    "print(\"x_test_text_review shape:\", x_test_text_review.shape)\n",
    "print(\"x_test_gen_summary shape:\", x_test_gen_summary.shape)\n",
    "print(\"test_ratings shape:\", test_ratings.shape)\n",
    "print(\"Vocabulary Size Text reviews: {:d}\".format(len(vocabulary_inv_text_review)))\n",
    "print(\"Vocabulary Size Generated Summary: {:d}\".format(len(vocabulary_inv_gen_summary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading emebdding vectors of pre-trained GoogleNews model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "filter_sizes = (3,4,5)\n",
    "num_filters = 50\n",
    "dropout_prob = (0.5, 0.5)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 40\n",
    "input_shape = (sequence_length_text_review,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = {}\n",
    "pretrained_fpath = os.path.expanduser(\"GoogleNews-vectors-negative300.bin\")\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(pretrained_fpath, binary=True)\n",
    "found_cnt = 0\n",
    "for id, word in vocabulary_inv_text_review.items():\n",
    "    if word in model.vocab:\n",
    "        embedding_weights[id] = model.word_vec(word)\n",
    "        found_cnt += 1\n",
    "    else:\n",
    "        embedding_weights[id] = np.random.uniform(-0.25, 0.25, embedding_dim)\n",
    "with open('gensim_embedding_weights', 'wb') as f:\n",
    "    pickle.dump(embedding_weights, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Review Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape=input_shape)\n",
    "\n",
    "z = Embedding(len(vocabulary_inv_text_review), embedding_dim, input_length=sequence_length_text_review, name=\"embedding\",trainable=True)(model_input)\n",
    "\n",
    "z = Dropout(dropout_prob[0])(z)\n",
    "\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(z)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "z = Dropout(dropout_prob[1])(z)\n",
    "z = Dense(hidden_dims, activation=\"relu\")(z)\n",
    "model_output = Dense(5, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = Model(model_input, model_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",keras.metrics.Precision(),keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 3768)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 3768, 300)    38305800    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 3768, 300)    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 3766, 50)     45050       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 3765, 50)     60050       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 3764, 50)     75050       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1883, 50)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1882, 50)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1882, 50)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 94150)        0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 94100)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 94100)        0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 282350)       0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 282350)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           14117550    dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 5)            255         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 52,603,755\n",
      "Trainable params: 52,603,755\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([v for v in embedding_weights.values()])\n",
    "embedding_layer = model.get_layer(\"embedding\")\n",
    "embedding_layer.set_weights([weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(filepath='models/cnn_text_review.hdf5', verbose=1, save_best_only=True, save_weights_only=True),\n",
    "    \n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1),\n",
    "    \n",
    "    EarlyStopping(monitor='val_loss', patience=5, verbose=1),\n",
    "\n",
    "    CSVLogger('./01-metrics.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janardhan/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 454741 samples, validate on 113686 samples\n",
      "Epoch 1/40\n",
      "454741/454741 [==============================] - 1531s 3ms/step - loss: 0.2580 - accuracy: 0.8986 - precision_1: 0.8180 - recall_1: 0.6342 - val_loss: 0.2349 - val_accuracy: 0.9071 - val_precision_1: 0.8595 - val_recall_1: 0.6403\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.23487, saving model to cnn_base_pretrained_1.hdf5\n",
      "Epoch 2/40\n",
      "454741/454741 [==============================] - 1527s 3ms/step - loss: 0.2185 - accuracy: 0.9115 - precision_1: 0.8404 - recall_1: 0.6882 - val_loss: 0.2164 - val_accuracy: 0.9149 - val_precision_1: 0.8575 - val_recall_1: 0.6891\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.23487 to 0.21643, saving model to cnn_base_pretrained_1.hdf5\n",
      "Epoch 3/40\n",
      "454741/454741 [==============================] - 1526s 3ms/step - loss: 0.2015 - accuracy: 0.9186 - precision_1: 0.8503 - recall_1: 0.7196 - val_loss: 0.2119 - val_accuracy: 0.9180 - val_precision_1: 0.8669 - val_recall_1: 0.6972\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.21643 to 0.21190, saving model to cnn_base_pretrained_1.hdf5\n",
      "Epoch 4/40\n",
      "454741/454741 [==============================] - 1528s 3ms/step - loss: 0.1884 - accuracy: 0.9240 - precision_1: 0.8590 - recall_1: 0.7414 - val_loss: 0.2025 - val_accuracy: 0.9209 - val_precision_1: 0.8593 - val_recall_1: 0.7228\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.21190 to 0.20251, saving model to cnn_base_pretrained_1.hdf5\n",
      "Epoch 5/40\n",
      " 81600/454741 [====>.........................] - ETA: 19:55 - loss: 0.1731 - accuracy: 0.9305 - precision_1: 0.8716 - recall_1: 0.7656"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275008/454741 [=================>............] - ETA: 9:35 - loss: 0.1675 - accuracy: 0.9331 - precision_1: 0.8751 - recall_1: 0.7763"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454741/454741 [==============================] - 1525s 3ms/step - loss: 0.1607 - accuracy: 0.9359 - precision_1: 0.8807 - recall_1: 0.7863 - val_loss: 0.1938 - val_accuracy: 0.9264 - val_precision_1: 0.8717 - val_recall_1: 0.7412\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.19591 to 0.19384, saving model to cnn_base_pretrained_1.hdf5\n",
      "Epoch 8/40\n",
      " 45376/454741 [=>............................] - ETA: 21:49 - loss: 0.1518 - accuracy: 0.9398 - precision_1: 0.8879 - recall_1: 0.7999"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235776/454741 [==============>...............] - ETA: 11:41 - loss: 0.1463 - accuracy: 0.9418 - precision_1: 0.8909 - recall_1: 0.8078"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451328/454741 [============================>.] - ETA: 10s - loss: 0.1429 - accuracy: 0.9436 - precision_1: 0.8948 - recall_1: 0.8139"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454741/454741 [==============================] - 1524s 3ms/step - loss: 0.1375 - accuracy: 0.9459 - precision_1: 0.8989 - recall_1: 0.8217 - val_loss: 0.1959 - val_accuracy: 0.9285 - val_precision_1: 0.8628 - val_recall_1: 0.7642\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.19275\n",
      "Epoch 12/40\n",
      "171584/454741 [==========>...................] - ETA: 15:04 - loss: 0.1305 - accuracy: 0.9486 - precision_1: 0.9037 - recall_1: 0.8319"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385344/454741 [========================>.....] - ETA: 3:42 - loss: 0.1207 - accuracy: 0.9526 - precision_1: 0.9119 - recall_1: 0.8448"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454741/454741 [==============================] - 1525s 3ms/step - loss: 0.1175 - accuracy: 0.9539 - precision_1: 0.9139 - recall_1: 0.8496 - val_loss: 0.1991 - val_accuracy: 0.9292 - val_precision_1: 0.8585 - val_recall_1: 0.7735\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.19275\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7ef3d429fbd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(x_test, y_test), verbose=1,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Review Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('models/cnn_text_review.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict(x_train_text_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(x_test_text_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_prediction_error(predicted_classes,labels):\n",
    "    error = 0\n",
    "    for index,i in enumerate(predicted_classes):\n",
    "        error += abs(labels[index]-i)\n",
    "    \n",
    "    return error/float(len(predicted_classes))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_average(predicted_classes,labels):\n",
    "    acc = 0\n",
    "    for index,i in enumerate(predicted_classes):\n",
    "        if(labels[index]==i):\n",
    "            acc += 1\n",
    "    \n",
    "    return acc/float(len(predicted_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpe = calc_mean_prediction_error(np.argmax(test_predictions,axis=1),np.argmax(test_ratings,axis=1))\n",
    "accuracy = calc_average(np.argmax(test_predictions,axis=1),np.argmax(test_ratings,axis=1))\n",
    "precision_recall = precision_recall_fscore_support(np.argmax(test_ratings,axis=1), np.argmax(test_predictions,axis=1), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for model based on Text reviews: 0.8093784634871488\n",
      "Mean Prediction Error for model based on Text reviews: 0.2774220220607639\n",
      "Precision for model based on Text reviews: 0.6814158475214385\n",
      "Recall for model based on Text reviews: 0.6466022289151423\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for model based on Text reviews:\",accuracy)\n",
    "print(\"Mean Prediction Error for model based on Text reviews:\",mpe)\n",
    "print(\"Precision for model based on Text reviews:\",precision_recall[0])\n",
    "print(\"Recall for model based on Text reviews:\",precision_recall[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training based on Generated summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = {}\n",
    "pretrained_fpath = os.path.expanduser(\"GoogleNews-vectors-negative300.bin\")\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(pretrained_fpath, binary=True)\n",
    "found_cnt = 0\n",
    "for id, word in vocabulary_inv_gen_summary.items():\n",
    "    if word in model.vocab:\n",
    "        embedding_weights[id] = model.word_vec(word)\n",
    "        found_cnt += 1\n",
    "    else:\n",
    "        embedding_weights[id] = np.random.uniform(-0.25, 0.25, embedding_dim)\n",
    "with open('gensim_embedding_weights', 'wb') as f:\n",
    "    pickle.dump(embedding_weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (sequence_length_gen_summary,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape=input_shape)\n",
    "\n",
    "z = Embedding(len(vocabulary_inv_gen_summary), embedding_dim, input_length=sequence_length_gen_summary, name=\"embedding\",trainable=True)(model_input)\n",
    "\n",
    "z = Dropout(dropout_prob[0])(z)\n",
    "\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(z)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "z = Dropout(dropout_prob[1])(z)\n",
    "z = Dense(hidden_dims, activation=\"relu\")(z)\n",
    "model_output = Dense(5, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = Model(model_input, model_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",keras.metrics.Precision(),keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([v for v in embedding_weights.values()])\n",
    "embedding_layer = model.get_layer(\"embedding\")\n",
    "embedding_layer.set_weights([weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 7)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 7, 300)       161100      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 7, 300)       0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5, 50)        45050       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 4, 50)        60050       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 3, 50)        75050       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 2, 50)        0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 2, 50)        0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 1, 50)        0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 100)          0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 100)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 50)           0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 250)          0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 250)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 50)           12550       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 5)            255         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 354,055\n",
      "Trainable params: 354,055\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(filepath='models/cnn_gen_summary.hdf5', verbose=1, save_best_only=True, save_weights_only=True),\n",
    "    \n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1),\n",
    "    \n",
    "    EarlyStopping(monitor='val_loss', patience=5, verbose=1),\n",
    "\n",
    "    CSVLogger('./01-metrics.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(x_test, y_test), verbose=1,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation - Generated Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('models/cnn_gen_summary.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict(x_train_gen_summary)\n",
    "test_predictions = model.predict(x_test_gen_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janardhan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "mpe = calc_mean_prediction_error(np.argmax(test_predictions,axis=1),np.argmax(test_ratings,axis=1))\n",
    "accuracy = calc_average(np.argmax(test_predictions,axis=1),np.argmax(test_ratings,axis=1))\n",
    "precision_recall = precision_recall_fscore_support(np.argmax(test_ratings,axis=1), np.argmax(test_predictions,axis=1), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for model based on Text reviews: 0.6831448023503334\n",
      "Mean Prediction Error for model based on Text reviews: 0.5998363914642084\n",
      "Precision for model based on Text reviews: 0.37091235139847417\n",
      "Recall for model based on Text reviews: 0.3536162549654081\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for model based on Generated Summary:\",accuracy)\n",
    "print(\"Mean Prediction Error for model based on Generated Summary:\",mpe)\n",
    "print(\"Precision for model based on Generated Summary:\",precision_recall[0])\n",
    "print(\"Recall for model based on Generated Summary:\",precision_recall[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
