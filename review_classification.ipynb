{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D, RepeatVector\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Important Functions</h1>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three cells are important functions dealing with data loading, models and evaluation metrics  \n",
    "**RUN the first three code cells**  \n",
    "  \n",
    "**Read review dataset**  \n",
    "Fields: ID, Product_ID, User_ID, Profile, HN, HD, Score, Time, Summary, Text  \n",
    "Fields needed: Score, Summary, text  \n",
    "  \n",
    "**Tokenization**  \n",
    "Build a vocabulary of max_words = 10000 most used words  \n",
    "Clip sentenses to maxlen = 200 words  \n",
    "One hot-encode words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load review data\n",
    "# text_type: 1 -> reviews full text, 0 -> generated summaries\n",
    "max_words = 10000\n",
    "\n",
    "def data_loader_reviews(text_type, maxlen = 200):\n",
    "    text = []\n",
    "    classes = []\n",
    "    score = []\n",
    "    head_line = True\n",
    "    with open(\"Reviews_summaries.csv\", 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        for row in csvreader:\n",
    "            if head_line:\n",
    "                head_line = False\n",
    "                continue\n",
    "            if(int(row[7]) >= 3):\n",
    "                classes.append(1)\n",
    "            else:\n",
    "                classes.append(0)\n",
    "\n",
    "            if text_type == 1:\n",
    "                text.append(row[10])\n",
    "            else:\n",
    "                text.append(row[11])\n",
    "                \n",
    "            score.append(int(row[7]))\n",
    "    \n",
    "    max_words = 10000\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    if text_type == 1:\n",
    "        sequences = sequence.pad_sequences(sequences, maxlen=maxlen)\n",
    "    else:\n",
    "        sequences = sequence.pad_sequences(sequences)\n",
    "\n",
    "    print(sequences[1])\n",
    "    return classes, score, sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Model**  \n",
    "define two RNN - models  \n",
    "  \n",
    "*Model 1:*  \n",
    "    a) sequential model with first layer as embedding layer followed by an LSTM layer and a sigmoid output for classifying review as positive-negative.  \n",
    "    b) sequential model with first layer as embedding layer followed by an LSTM layer and a softmax output with 2 nodes for classifying review as positive-negative.  \n",
    "  \n",
    "*Model 2:*  \n",
    "sequential model with first layer as embedding layer followed by an LSTM layer and a softmax output with 5 nodes for classifying review on a scale of 1-5.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_neg_model(case):\n",
    "    m = Sequential()\n",
    "    m.add(Embedding(max_words, 32))\n",
    "    m.add(LSTM(32))\n",
    "    \n",
    "    if case == 1:  # sigmoid output\n",
    "        m.add(Dense(1, activation='sigmoid'))\n",
    "        m.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "    else:\n",
    "        m.add(Dense(2, activation='softmax'))\n",
    "        m.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    m.summary()\n",
    "    return m\n",
    "\n",
    "def rating_model():\n",
    "    m = Sequential()\n",
    "    m.add(Embedding(max_words, 32))\n",
    "    m.add(LSTM(32))\n",
    "    m.add(Dense(5, activation='softmax'))\n",
    "    m.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    m.summary()\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall(predicted, actual):\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for p, a in zip(predicted, actual):\n",
    "        p = round(p)\n",
    "        \n",
    "        if a==1 and p==1:\n",
    "            tp += 1\n",
    "        elif a==1 and p==0:\n",
    "            fn += 1\n",
    "        elif a ==0 and p==1:\n",
    "            fp += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "    return precision, recall, accuracy\n",
    "\n",
    "def MAE(predicted, actual):\n",
    "    mae = 0\n",
    "    for p, a in zip(predicted, actual):\n",
    "        mae += abs(a - p)\n",
    "    mae = mae/len(predicted)\n",
    "    return mae\n",
    "\n",
    "def accuracy(predicted, actual):\n",
    "    acc = 0\n",
    "    for p, a in zip(predicted, actual):\n",
    "        if a == p:\n",
    "            acc += 1\n",
    "    acc = acc/len(predicted)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<h1>Part1: Classifying reviews as positive/negative</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train** model for predicting pos/neg  \n",
    "Steps:  \n",
    "1. Load and tokenize data\n",
    "2. Split data into training and testing set in ratio 4:1\n",
    "3. Select the model type. type = 1 (default, preferred) for sigmoid output layer.\n",
    "4. Train the model\n",
    "5. Save the weights\n",
    "6. If not training, then load the weights. Model weights are provided for type = 1 and are selected by default\n",
    "7. Predict class for test data and evaluate\n",
    "\n",
    "*Complete Part1 takes around 2.5 hours to execute on a 1070TI GPU*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   38  376 2210   23 5763 1961 1083    1 1083   78  252  193 1054 3613\n",
      "   18  209   36    9   21   71 3190   31   36    1 1593 2218    5 7946\n",
      "    1   38   23 5763]\n",
      "Train Set: 454763\n",
      "Test Set: 113691\n"
     ]
    }
   ],
   "source": [
    "# Split dataset to test and train\n",
    "review_classes, review_rating, review_text = data_loader_reviews(text_type = 1)\n",
    "text_train, text_test, class_train, class_test = train_test_split(review_text, review_classes, test_size=0.20)\n",
    "print(\"Train Set:\", len(text_train))\n",
    "print(\"Test Set:\", len(text_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 328,386\n",
      "Trainable params: 328,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "One Hot Encode classes\n",
      "Start Training\n",
      "Train on 304691 samples, validate on 150072 samples\n",
      "Epoch 1/10\n",
      "304691/304691 [==============================] - 477s 2ms/step - loss: 0.2297 - acc: 0.9072 - val_loss: 0.2022 - val_acc: 0.9164\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91637, saving model to weights-best.h5\n",
      "Epoch 2/10\n",
      "304691/304691 [==============================] - 476s 2ms/step - loss: 0.1862 - acc: 0.9250 - val_loss: 0.1783 - val_acc: 0.9287\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.91637 to 0.92867, saving model to weights-best.h5\n",
      "Epoch 3/10\n",
      "304691/304691 [==============================] - 486s 2ms/step - loss: 0.1676 - acc: 0.9328 - val_loss: 0.1782 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.92867\n",
      "Epoch 4/10\n",
      "304691/304691 [==============================] - 461s 2ms/step - loss: 0.1550 - acc: 0.9383 - val_loss: 0.1608 - val_acc: 0.9349\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.92867 to 0.93494, saving model to weights-best.h5\n",
      "Epoch 5/10\n",
      "304691/304691 [==============================] - 457s 1ms/step - loss: 0.1443 - acc: 0.9426 - val_loss: 0.1535 - val_acc: 0.9385\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.93494 to 0.93854, saving model to weights-best.h5\n",
      "Epoch 6/10\n",
      "304691/304691 [==============================] - 475s 2ms/step - loss: 0.1350 - acc: 0.9465 - val_loss: 0.1555 - val_acc: 0.9381\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.93854\n",
      "Epoch 7/10\n",
      "304691/304691 [==============================] - 483s 2ms/step - loss: 0.1273 - acc: 0.9500 - val_loss: 0.1470 - val_acc: 0.9422\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.93854 to 0.94217, saving model to weights-best.h5\n",
      "Epoch 8/10\n",
      "304691/304691 [==============================] - 503s 2ms/step - loss: 0.1207 - acc: 0.9531 - val_loss: 0.1524 - val_acc: 0.9386\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.94217\n",
      "Epoch 9/10\n",
      "304691/304691 [==============================] - 517s 2ms/step - loss: 0.1149 - acc: 0.9557 - val_loss: 0.1653 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.94217\n",
      "Epoch 10/10\n",
      "304691/304691 [==============================] - 487s 2ms/step - loss: 0.1099 - acc: 0.9575 - val_loss: 0.1484 - val_acc: 0.9412\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.94217\n"
     ]
    }
   ],
   "source": [
    "# model_type: 1 -> sigmoid output, 2 -> softmax with two nodes output\n",
    "model_type = 1\n",
    "model = pos_neg_model(model_type)\n",
    "\n",
    "filepath=\"models/weights-best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "if model_type == 1:\n",
    "    history = model.fit(text_train, class_train, epochs=10, batch_size=128, validation_split=0.2, callbacks=callbacks_list)\n",
    "else:\n",
    "    # one hot encode classes\n",
    "    print(\"One Hot Encode classes\")\n",
    "    classes = []\n",
    "    for c in class_train:\n",
    "        temp = [0, 0]\n",
    "        temp[c] = 1\n",
    "        classes.append(temp)\n",
    "    print(\"Start Training\")\n",
    "    classes = np.array(classes)\n",
    "    history = model.fit(text_train, classes, epochs=10, batch_size=128, validation_split=0.33, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE WEIGHTS\n",
    "if model_type == 1:\n",
    "    model.save_weights(\"models/rnn-pos-neg-model-sigmoid.h5\")\n",
    "else:\n",
    "    model.save_weights(\"models/rnn-pos-neg-model-softmax.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD WEIGHTS\n",
    "if model_type == 1:\n",
    "    model = pos_neg_model(1)\n",
    "    model.load_weights(\"models/rnn-pos-neg-model-sigmoid.h5\")\n",
    "else:\n",
    "    model = pos_neg_model(2)\n",
    "    model.load_weights(\"models/rnn-pos-neg-model-softmax.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EVALUATION** of pos-neg classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot Encode classes\n",
      "113691/113691 [==============================] - 102s 893us/step\n",
      "[0.14725378104959091, 0.9409715808691633]\n"
     ]
    }
   ],
   "source": [
    "if model_type == 1:\n",
    "    scores = model.evaluate(text_test, class_test)\n",
    "else:\n",
    "    print(\"One Hot Encode classes\")\n",
    "    classes = []\n",
    "    for c in class_test:\n",
    "        temp = [0, 0]\n",
    "        temp[c] = 1\n",
    "        classes.append(temp)\n",
    "    classes = np.array(classes)\n",
    "    scores = model.evaluate(text_test, classes)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113691\n",
      "One Hot Encode classes\n",
      "Precision: 0.9491320305525246\n",
      "Recall: 0.9837248753405644\n",
      "Accuracy: 0.9409715808639206\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(text_test)\n",
    "print(len(pred))\n",
    "if model_type == 1:\n",
    "    preds_classes = np.argmax(pred, axis=-1)\n",
    "    results = precision_recall(pred_classes, class_test)\n",
    "else:\n",
    "    print(\"One Hot Encode classes\")\n",
    "    classes = []\n",
    "    for c in class_test:\n",
    "        temp = [0, 0]\n",
    "        temp[c] = 1\n",
    "        classes.append(temp)\n",
    "    preds_classes = np.argmax(pred, axis=-1)\n",
    "    results = precision_recall(preds_classes, class_test)\n",
    "\n",
    "print(\"Precision:\", results[0])\n",
    "print(\"Recall:\", results[1])\n",
    "print(\"Accuracy:\", results[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS** for LSTM with sigmoid activation in last layer  \n",
    "Precision: 0.9709278222295925  \n",
    "Recall: 0.9631316506451447  \n",
    "Accuracy: 0.9437862275817787  \n",
    "\n",
    "**RESULTS** for LSTM with softmax last layer  \n",
    "Precision: 0.9491320305525246  \n",
    "Recall: 0.9837248753405644  \n",
    "Accuracy: 0.9409715808639206  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<h1>Part2: Rating prediction for reviews (1 - 5)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train** model for predicting ratings  \n",
    "Steps:  \n",
    "1. Load and tokenize data. text_type = 1 (default, preferred) for full text reviews, 2 for summaries\n",
    "2. Split data into training and testing set in ratio 4:1\n",
    "3. Select the model type.\n",
    "4. Train the model\n",
    "5. Save the weights\n",
    "6. If not training, then load the weights. Model weights are provided and are selected by default\n",
    "7. Predict class for test data and evaluate\n",
    "\n",
    "*Complete Part2 takes around 4 hours to execute on a 1070TI GPU*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0 176]\n",
      "Train Set: 454763\n",
      "Test Set: 113691\n"
     ]
    }
   ],
   "source": [
    "# Split dataset to test and train\n",
    "text_type = 2  # 1 -> full length review, 2 -> short summaries\n",
    "review_classes, review_rating, review_text = data_loader_reviews(text_type = text_type, maxlen = 100)\n",
    "text_train, text_test, class_train, class_test = train_test_split(review_text, review_rating, test_size=0.20)\n",
    "print(\"Train Set:\", len(text_train))\n",
    "print(\"Test Set:\", len(text_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 328,485\n",
      "Trainable params: 328,485\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "One Hot Encode classes\n",
      "Start Training\n",
      "Train on 363810 samples, validate on 90953 samples\n",
      "Epoch 1/10\n",
      "363810/363810 [==============================] - 221s 608us/step - loss: 0.8169 - acc: 0.6982 - val_loss: 0.7292 - val_acc: 0.7259\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.72591, saving model to rating-weights-best.h5\n",
      "Epoch 2/10\n",
      "363810/363810 [==============================] - 235s 647us/step - loss: 0.7132 - acc: 0.7327 - val_loss: 0.7314 - val_acc: 0.7199\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.72591\n",
      "Epoch 3/10\n",
      "363810/363810 [==============================] - 235s 645us/step - loss: 0.6771 - acc: 0.7474 - val_loss: 0.6925 - val_acc: 0.7416\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.72591 to 0.74158, saving model to rating-weights-best.h5\n",
      "Epoch 4/10\n",
      "363810/363810 [==============================] - 232s 639us/step - loss: 0.6476 - acc: 0.7593 - val_loss: 0.6796 - val_acc: 0.7478\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.74158 to 0.74783, saving model to rating-weights-best.h5\n",
      "Epoch 5/10\n",
      "363810/363810 [==============================] - 240s 661us/step - loss: 0.6211 - acc: 0.7698 - val_loss: 0.6468 - val_acc: 0.7590\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.74783 to 0.75899, saving model to rating-weights-best.h5\n",
      "Epoch 6/10\n",
      "363810/363810 [==============================] - 243s 667us/step - loss: 0.5981 - acc: 0.7789 - val_loss: 0.6294 - val_acc: 0.7669\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.75899 to 0.76691, saving model to rating-weights-best.h5\n",
      "Epoch 7/10\n",
      "363810/363810 [==============================] - 235s 647us/step - loss: 0.5770 - acc: 0.7876 - val_loss: 0.6277 - val_acc: 0.7674\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.76691 to 0.76744, saving model to rating-weights-best.h5\n",
      "Epoch 8/10\n",
      "363810/363810 [==============================] - 229s 628us/step - loss: 0.5580 - acc: 0.7950 - val_loss: 0.6593 - val_acc: 0.7542\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.76744\n",
      "Epoch 9/10\n",
      "363810/363810 [==============================] - 228s 627us/step - loss: 0.5411 - acc: 0.8022 - val_loss: 0.6526 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.76744\n",
      "Epoch 10/10\n",
      "363810/363810 [==============================] - 230s 632us/step - loss: 0.5252 - acc: 0.8086 - val_loss: 0.6173 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.76744 to 0.77597, saving model to rating-weights-best.h5\n"
     ]
    }
   ],
   "source": [
    "model = rating_model()\n",
    "\n",
    "filepath=\"rating-weights-best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "# one hot encode classes\n",
    "print(\"One Hot Encode classes\")\n",
    "classes = []\n",
    "for c in class_train:\n",
    "    temp = [0]*5\n",
    "    temp[c-1] = 1\n",
    "    classes.append(temp)\n",
    "print(\"Start Training\")\n",
    "classes = np.array(classes)\n",
    "history = model.fit(text_train, classes, epochs=10, batch_size=128, validation_split=0.2, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE WEIGHTS\n",
    "model.save_weights(\"models/rnn-rating-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 328,485\n",
      "Trainable params: 328,485\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LOAD MODEL\n",
    "model = rating_model()\n",
    "model.load_weights(\"models/rnn-rating-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113691\n",
      "One Hot Encode classes\n",
      "[5 5 5 ... 3 5 1]\n",
      "MAE: 0.5979453078959637\n",
      "Accuracy: 0.6821384278438927\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(text_test)\n",
    "print(len(pred))\n",
    "\n",
    "print(\"One Hot Encode classes\")\n",
    "pred_classes = np.argmax(pred, axis=-1)\n",
    "pred_classes = pred_classes + 1\n",
    "print(pred_classes)\n",
    "# results = precision_recall(pred_classes, class_test)\n",
    "mae = MAE(pred_classes, class_test)\n",
    "acc = accuracy(pred_classes, class_test)\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS** for LSTM with full text reviews  \n",
    "MAE: 0.3116341662928464  \n",
    "Accuracy: 0.7860428705878214  \n",
    "  \n",
    "**RESULTS** for LSTM with summary review  \n",
    "MAE: 0.5979453078959637  \n",
    "Accuracy: 0.6821384278438927  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
