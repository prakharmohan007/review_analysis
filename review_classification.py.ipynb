{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D, RepeatVector\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB dataset: To make our review dataset compatible\n",
    "max_features = 10000\n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(input_train), 'train sequences')\n",
    "print(len(input_test), 'test sequences')\n",
    "print(input_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read review dataset  \n",
    "Fields: ID, Product_ID, User_ID, Profile, HN, HD, Score, Time, Summary, Text  \n",
    "Fields needed: Score, Summary, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 568454\n",
      "maximum length review: 3529\n",
      "vocabulary size: 159169\n"
     ]
    }
   ],
   "source": [
    "# data = []\n",
    "score = []\n",
    "# summary = []\n",
    "text = []\n",
    "review_class = []\n",
    "vocabulary = dict()\n",
    "num_words = 1\n",
    "maxlen = 0\n",
    "head_line = True\n",
    "with open(\"Reviews.csv\", 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    for row in csvreader:\n",
    "        if head_line:\n",
    "            head_line = False\n",
    "            continue\n",
    "        # data.append(row)\n",
    "        score.append(int(row[6]))\n",
    "        if(int(row[6]) >= 3):\n",
    "            review_class.append(1)\n",
    "        else:\n",
    "            review_class.append(0)\n",
    "        \n",
    "        # summary.append(row[8])\n",
    "        text.append([])\n",
    "        review = re.findall('\\w+', row[9])\n",
    "        maxlen = max(maxlen, len(review))\n",
    "        for w in review:\n",
    "            if w not in vocabulary:\n",
    "                vocabulary[w] = num_words\n",
    "                num_words += 1\n",
    "            text[-1].append(vocabulary[w])\n",
    "        del review\n",
    "        \n",
    "maxfeat = len(vocabulary)        \n",
    "print(\"Number of samples:\", len(text))\n",
    "# print(text[1])\n",
    "print(\"maximum length review:\", maxlen)\n",
    "print(\"vocabulary size:\", maxfeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38, 376, 2210, 23, 5763, 1961, 1083, 1, 1083, 78, 252, 193, 1054, 3613, 18, 209, 36, 9, 21, 71, 3190, 31, 36, 1, 1593, 2218, 5, 7946, 1, 38, 23, 5763]\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "reviews = []\n",
    "review_class = []\n",
    "score = []\n",
    "head_line = True\n",
    "with open(\"Reviews.csv\", 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    for row in csvreader:\n",
    "        if head_line:\n",
    "            head_line = False\n",
    "            continue\n",
    "        # data.append(row)\n",
    "        # score.append(row[6])\n",
    "        if(int(row[6]) >= 3):\n",
    "            review_class.append(1)\n",
    "        else:\n",
    "            review_class.append(0)\n",
    "            \n",
    "        reviews.append(row[9])\n",
    "        score.append(int(row[6]))\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "print(sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 133039\n"
     ]
    }
   ],
   "source": [
    "print('vocabulary size:',len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: 454763\n",
      "Test Set: 113691\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    2  125   13  106   46    7 7700  265  475\n",
      "   32    1  249 5422    5  101    9  164   57    2   21 1311  602   74\n",
      "   32  340 3119    4   46    2 1065    9   56   30    4  708  293  164\n",
      "   44   18   10   10  435    6 3146    1 2379 3607   16  999 8754   13\n",
      " 1496  288  266   29    3 1527 8140    4 3654  105   15   32    4   69\n",
      "   48 1615   87    6   21   25 1527    2  185  212   13 1025  483  105\n",
      "   10   10  435    9   40  196   17    1   42    7    4    2   56  136\n",
      "  146    9  164   11  233  137 1300    4  203  105    7   40   12    1\n",
      "  312 1332    6 1426    5 1120    9   40    8  120   11 8248  135    3\n",
      "   14  849  203    3  453   10   10  677    9   40    8  136  430   32\n",
      "   13  249 5422   58  261   65   89   16    1 3854  222  392  392 2737\n",
      " 2897  218   12 4659]\n"
     ]
    }
   ],
   "source": [
    "maxlen = 200\n",
    "sequences = sequence.pad_sequences(sequences, maxlen=maxlen)\n",
    "text_train, text_test, class_train, class_test = train_test_split(sequences, review_class, test_size=0.20)\n",
    "print(\"Train Set:\", len(text_train))\n",
    "print(\"Test Set:\", len(text_test))\n",
    "print(text_train[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_neg_model(case):\n",
    "    m = Sequential\n",
    "    m = Sequential()\n",
    "    m.add(Embedding(max_words, 32))\n",
    "    m.add(LSTM(32))\n",
    "    \n",
    "    if case == 1:  # sigmoid output\n",
    "        m.add(Dense(1, activation='sigmoid'))\n",
    "        m.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "    else:\n",
    "        m.add(Dense(2, activation='softmax'))\n",
    "        m.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    m.summary()\n",
    "    return m\n",
    "\n",
    "def rating_model():\n",
    "    m = Sequential\n",
    "    m = Sequential()\n",
    "    m.add(Embedding(max_words, 32))\n",
    "    m.add(LSTM(32))\n",
    "    m.add(Dense(5, activation='softmax'))\n",
    "    m.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    m.summary()\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 328,386\n",
      "Trainable params: 328,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "One Hot Encode classes\n",
      "Start Training\n",
      "Train on 304691 samples, validate on 150072 samples\n",
      "Epoch 1/10\n",
      "304691/304691 [==============================] - 477s 2ms/step - loss: 0.2297 - acc: 0.9072 - val_loss: 0.2022 - val_acc: 0.9164\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91637, saving model to weights-best.h5\n",
      "Epoch 2/10\n",
      "304691/304691 [==============================] - 476s 2ms/step - loss: 0.1862 - acc: 0.9250 - val_loss: 0.1783 - val_acc: 0.9287\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.91637 to 0.92867, saving model to weights-best.h5\n",
      "Epoch 3/10\n",
      "304691/304691 [==============================] - 486s 2ms/step - loss: 0.1676 - acc: 0.9328 - val_loss: 0.1782 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.92867\n",
      "Epoch 4/10\n",
      "304691/304691 [==============================] - 461s 2ms/step - loss: 0.1550 - acc: 0.9383 - val_loss: 0.1608 - val_acc: 0.9349\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.92867 to 0.93494, saving model to weights-best.h5\n",
      "Epoch 5/10\n",
      "304691/304691 [==============================] - 457s 1ms/step - loss: 0.1443 - acc: 0.9426 - val_loss: 0.1535 - val_acc: 0.9385\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.93494 to 0.93854, saving model to weights-best.h5\n",
      "Epoch 6/10\n",
      "304691/304691 [==============================] - 475s 2ms/step - loss: 0.1350 - acc: 0.9465 - val_loss: 0.1555 - val_acc: 0.9381\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.93854\n",
      "Epoch 7/10\n",
      "304691/304691 [==============================] - 483s 2ms/step - loss: 0.1273 - acc: 0.9500 - val_loss: 0.1470 - val_acc: 0.9422\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.93854 to 0.94217, saving model to weights-best.h5\n",
      "Epoch 8/10\n",
      "304691/304691 [==============================] - 503s 2ms/step - loss: 0.1207 - acc: 0.9531 - val_loss: 0.1524 - val_acc: 0.9386\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.94217\n",
      "Epoch 9/10\n",
      "304691/304691 [==============================] - 517s 2ms/step - loss: 0.1149 - acc: 0.9557 - val_loss: 0.1653 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.94217\n",
      "Epoch 10/10\n",
      "304691/304691 [==============================] - 487s 2ms/step - loss: 0.1099 - acc: 0.9575 - val_loss: 0.1484 - val_acc: 0.9412\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.94217\n"
     ]
    }
   ],
   "source": [
    "# filepath=\"weights-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "model_type = 2\n",
    "model = pos_neg_model(model_type)\n",
    "\n",
    "filepath=\"weights-best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "if model_type == 1:\n",
    "    history = model.fit(text_train, class_train, epochs=10, batch_size=128, validation_split=0.2, callbacks=callbacks_list)\n",
    "else:\n",
    "    # one hot encode classes\n",
    "    print(\"One Hot Encode classes\")\n",
    "    classes = []\n",
    "    for c in class_train:\n",
    "        temp = [0, 0]\n",
    "        temp[c] = 1\n",
    "        classes.append(temp)\n",
    "    print(\"Start Training\")\n",
    "    classes = np.array(classes)\n",
    "    history = model.fit(text_train, classes, epochs=10, batch_size=128, validation_split=0.33, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"weights-best.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weights-best.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot Encode classes\n",
      "113691/113691 [==============================] - 102s 893us/step\n",
      "[0.14725378104959091, 0.9409715808691633]\n"
     ]
    }
   ],
   "source": [
    "if model_type == 1:\n",
    "    scores = model.evaluate(text_test, class_test)\n",
    "else:\n",
    "    print(\"One Hot Encode classes\")\n",
    "    classes = []\n",
    "    for c in class_test:\n",
    "        temp = [0, 0]\n",
    "        temp[c] = 1\n",
    "        classes.append(temp)\n",
    "    classes = np.array(classes)\n",
    "    scores = model.evaluate(text_test, classes)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113691\n",
      "One Hot Encode classes\n",
      "Precision: 0.9491320305525246\n",
      "Recall: 0.9837248753405644\n",
      "Accuracy: 0.9409715808639206\n"
     ]
    }
   ],
   "source": [
    "def precision_recall(predicted, actual):\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for p, a in zip(predicted, actual):\n",
    "        p = round(p)\n",
    "        \n",
    "        if a==1 and p==1:\n",
    "            tp += 1\n",
    "        elif a==1 and p==0:\n",
    "            fn += 1\n",
    "        elif a ==0 and p==1:\n",
    "            fp += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "    return precision, recall, accuracy\n",
    "\n",
    "print(len(pred))\n",
    "if model_type == 1:\n",
    "    preds_classes = np.argmax(pred, axis=-1)\n",
    "    results = precision_recall(pred_classes, class_test)\n",
    "else:\n",
    "    print(\"One Hot Encode classes\")\n",
    "    classes = []\n",
    "    for c in class_test:\n",
    "        temp = [0, 0]\n",
    "        temp[c] = 1\n",
    "        classes.append(temp)\n",
    "    preds_classes = np.argmax(pred, axis=-1)\n",
    "    results = precision_recall(preds_classes, class_test)\n",
    "\n",
    "print(\"Precision:\", results[0])\n",
    "print(\"Recall:\", results[1])\n",
    "print(\"Accuracy:\", results[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS** for LSTM with sigmoid activation in last layer  \n",
    "Precision: 0.9709278222295925  \n",
    "Recall: 0.9631316506451447  \n",
    "Accuracy: 0.9437862275817787  \n",
    "\n",
    "**RESULTS** for LSTM with softmax last layer  \n",
    "Precision: 0.9491320305525246  \n",
    "Recall: 0.9837248753405644  \n",
    "Accuracy: 0.9409715808639206  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: 454763\n",
      "Test Set: 113691\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    2  575  178  192\n",
      "   29  320   64    9   38    2   91 1297    5   28]\n"
     ]
    }
   ],
   "source": [
    "maxlen = 500\n",
    "sequences = sequence.pad_sequences(sequences, maxlen=maxlen)\n",
    "text_train, text_test, rating_train, rating_test = train_test_split(sequences, score, test_size=0.20)\n",
    "print(\"Train Set:\", len(text_train))\n",
    "print(\"Test Set:\", len(text_test))\n",
    "print(text_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 328,485\n",
      "Trainable params: 328,485\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "One Hot Encode classes\n",
      "Start Training\n",
      "Train on 304691 samples, validate on 150072 samples\n",
      "Epoch 1/10\n",
      "304691/304691 [==============================] - 968s 3ms/step - loss: 0.8313 - acc: 0.6925 - val_loss: 0.8729 - val_acc: 0.6869\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.68689, saving model to rating-weights-best.h5\n",
      "Epoch 2/10\n",
      "304691/304691 [==============================] - 940s 3ms/step - loss: 0.7168 - acc: 0.7315 - val_loss: 0.7560 - val_acc: 0.7232\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.68689 to 0.72317, saving model to rating-weights-best.h5\n",
      "Epoch 3/10\n",
      "304691/304691 [==============================] - 956s 3ms/step - loss: 0.6802 - acc: 0.7460 - val_loss: 0.6852 - val_acc: 0.7452\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.72317 to 0.74523, saving model to rating-weights-best.h5\n",
      "Epoch 4/10\n",
      "304691/304691 [==============================] - 999s 3ms/step - loss: 0.6471 - acc: 0.7592 - val_loss: 0.6842 - val_acc: 0.7486\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.74523 to 0.74861, saving model to rating-weights-best.h5\n",
      "Epoch 5/10\n",
      "187008/304691 [=================>............] - ETA: 5:50 - loss: 0.6189 - acc: 0.7700"
     ]
    }
   ],
   "source": [
    "model = rating_model()\n",
    "\n",
    "filepath=\"rating-weights-best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# one hot encode classes\n",
    "print(\"One Hot Encode classes\")\n",
    "classes = []\n",
    "for c in rating_train:\n",
    "    temp = [0]*5\n",
    "    temp[c-1] = 1\n",
    "    classes.append(temp)\n",
    "print(\"Start Training\")\n",
    "classes = np.array(classes)\n",
    "history = model.fit(text_train, classes, epochs=10, batch_size=128, validation_split=0.33, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
